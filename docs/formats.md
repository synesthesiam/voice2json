# Data Formats

`voice2json` strives to use only common data formats, preferrably text-based. Some artifacts generated during [training](commands.md#train-profile), such as your [language model](#language-models), are even usable by [other speech systems](https://github.com/mozilla/DeepSpeech).

## Audio

`voice2json` expects 16-bit 16Khz mono audio as input. When WAV data is provided in a different format, it is automatically converted with [sox](http://sox.sourceforge.net).

## Transcriptions

The [transcribe-wav](commands.md#transcribe-wav) command produces JSON in the following format:

```json
{
    "text": "transcription text",
    "transcribe_seconds": 0.123,
    "wav_name": "test.wav",
    "wav_seconds": 1.456
}
```

where

* `text` is the most likely transcription of the audio data (string)
* `transcribe_seconds` is the number of seconds it took to transcribe (number)
* `wav_name` is the name of the WAV file (string)
* `wav_seconds` is the duration of the WAV audio in seconds (number)

## Intents

The [recognize-intent](commands.md#recognize-intent) command produces JSON in the following format:

```json
{
    "intent": {
        "name": "NameOfIntent",
        "confidence": 1.0
    },
    "entities": [
        { "entity": "entity_1", "value": "value_1", "raw_value": "value_1",
          "start": 0, "end": 1, "raw_start": 0, "raw_end": 1 },
        { "entity": "entity_2", "value": "value_2", "raw_value": "value_2",
          "start": 0, "end": 1, "raw_start": 0, "raw_end": 1 }
    ],
    "slots": {
        "entity_1": "value_1",
        "entity_2": "value_2"
    },
    "text": "transcription text with substitutions",
    "raw_text": "transcription text without substitutions",
    "tokens": ["transcription", "text", "with", "substitutions"],
    "raw_tokens": ["transcription", "text", "without", "substitutions"],
    "recognize_seconds": 0.001
    
}
```

where

* `intent` describes the recognized intent (object)
    * `name` is the name of the recognized intent (section headers in your [sentences.ini](sentences.md)) (string)
    * `confidence` is a value between 0 and 1, with 1 being maximally confident (number)
* `entities` is a list of recognized entities (list)
    * `entity` is the name of the slot (string)
    * `value` is the ([substitued](sentences.md#wordtag-substitutions)) value (string)
    * `raw_value` is the (**non**-substitued) value (string)
    * `start` is the zero-based start index of the entity in `text` (number)
    * `raw_start` is the zero-based start index of the entity in `raw_text` (number)
    * `stop` is the zero-based stop index (exclusive) of the entity in `text` (number)
    * `raw_stop` is the zero-based stop index (exclusive) of the entity in `raw_text` (number)
* `slots` is a dictionary of entities/values (object)
    * Assumes one value per entity. See `entities` for complete list.
* `text` is the input text with [substitutions](sentences.md#wordtag-substitutions) (string)
* `raw_text` is the input text **without** substitutions
* `tokens` is the list of words/tokens in `text`
* `raw_tokens` is the list of words/tokens in `raw_text`
* `recognize_seconds` is the number of seconds it took to recognize the intent and slots (number)

## Pronunciation Dictionaries

Dictionaries are expected in plaintext, with the following format:

```
word1 P1 P2 P3
word2 P1 P4 P5
...
```

Each line starts with a word and, after some whitespace, a list of phonemes are given (separated by whitespace). These phonemes must match what the acoustic model was trained to recognize.

Multiple pronunciations for the same word are possible, and may optionally contain an index:

```
word P1 P2 P3
word(1) P2 P2 P3
word(2) P3 P2 P3
```

A `voice2json` [profile](profiles.md) will typically contain 3 dictionaries:

1. `base_dictionary.txt`
    * A large, pre-built dictionary with most of the words in a given language
2. `custom_words.txt`
    * A small, user-defined dictionary with custom words or pronunciations
3. `dictionary.txt`
    * Contains exactly the vocabulary needed for a profile
    * Automatically generated by [train-profile](commands.md#train-profile)

## Language Models

Language models must be in plaintext [ARPA format](https://cmusphinx.github.io/wiki/arpaformat/).

A `voice2json` [profile](profiles.md) will typically contain 2 language models:

1. `base_language_model.txt`
    * A large, pre-built language model that summarizes a given language
    * Used when `--open` flag is given to [transcribe-wav](commands.md#transcribe-wav)
    * Used during [language model mixing](commands.md#language-model-mixing)
2. `language_model.txt`
    * Summarizes the valid voice commands for a profile
    * Automatically generated by [train-profile](commands.md#train-profile)
    
    
## Grapheme To Phoneme Models

A grapheme-to-phoneme (g2p) model helps guess the pronunciations of words outside of [the dictionary](#pronunciation-dictionaries). These models are trained on each profile's `base_dictionary.txt` file using [phonetisaurus](https://github.com/AdolfVonKleist/Phonetisaurus) and saved in the [OpenFST](http://www.openfst.org) binary format.

## eSpeak Phoneme Maps

Each profile contains an `espeak_phonemes.txt` file that contains a mapping from the phonemes present in the profile's [pronunciation dictionaries](#pronunciations-dictionaries) and the phonemes that [eSpeak](http://espeak.sourceforge.net) itself uses to pronounce words. The format is simple:

```
P1 E1
P2 E2
...
```

where `P1` is a dictionary phoneme and `E1` is an eSpeak phoneme. These mappings are produced manually, and may not be perfect. The goal is to help users [hear how the speech recognizer is expecting a word to be pronounced](commands.md#pronounce-word).
